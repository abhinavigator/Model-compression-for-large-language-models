{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch.nn.utils.prune as prune\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T04:52:41.157018Z","iopub.execute_input":"2024-05-02T04:52:41.157359Z","iopub.status.idle":"2024-05-02T04:53:02.554847Z","shell.execute_reply.started":"2024-05-02T04:52:41.157331Z","shell.execute_reply":"2024-05-02T04:53:02.553946Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-02 04:52:52.759275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-02 04:52:52.759371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-02 04:52:52.933022: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\n\nclass SummarizationDataset(Dataset):\n    def __init__(self, tokenizer, file_path, max_length=512, use_percentage=10):\n        self.dataframe = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Sample a percentage of the data if use_percentage is less than 100\n        if use_percentage < 100:\n            self.dataframe = self.dataframe.sample(frac=use_percentage / 100.0, random_state=42).reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        article_text = row['article']\n        highlights_text = row['highlights']\n        \n        input_text = f\"summarize: {article_text}\"\n        source_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        target_encoding = self.tokenizer(\n            highlights_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': source_encoding['input_ids'].squeeze(),\n            'attention_mask': source_encoding['attention_mask'].squeeze(),\n            'labels': target_encoding['input_ids'].squeeze()\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:53:48.281804Z","iopub.execute_input":"2024-05-02T04:53:48.282896Z","iopub.status.idle":"2024-05-02T04:53:48.292904Z","shell.execute_reply.started":"2024-05-02T04:53:48.282860Z","shell.execute_reply":"2024-05-02T04:53:48.291857Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:53:49.731279Z","iopub.execute_input":"2024-05-02T04:53:49.731967Z","iopub.status.idle":"2024-05-02T04:53:52.695163Z","shell.execute_reply.started":"2024-05-02T04:53:49.731935Z","shell.execute_reply":"2024-05-02T04:53:52.694273Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac813313a764aedafeea50739194968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2924af0b6c5406997ea155e79cd8263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a61353e8d24a484e9792df43e0c13cf2"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dc840a70f444e89211ba8a8ee8760d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28edccd907214aa08e2fb10fbd2f104c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a6019a16754a449590c5f96171aa0b"}},"metadata":{}}]},{"cell_type":"code","source":"def apply_pruning(model):\n    # Iterate over all modules and prune the linear layers found in the encoder and decoder\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            # Applying unstructured L1 pruning\n            prune.l1_unstructured(module, name='weight', amount=0.2)\n            # To make the pruning permanent, you might typically call prune.remove, but it is better to do it after training\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:53:52.696662Z","iopub.execute_input":"2024-05-02T04:53:52.696943Z","iopub.status.idle":"2024-05-02T04:53:52.701797Z","shell.execute_reply.started":"2024-05-02T04:53:52.696919Z","shell.execute_reply":"2024-05-02T04:53:52.700999Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Pruning before training\napply_pruning(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:53:56.528987Z","iopub.execute_input":"2024-05-02T04:53:56.529581Z","iopub.status.idle":"2024-05-02T04:53:58.770366Z","shell.execute_reply.started":"2024-05-02T04:53:56.529549Z","shell.execute_reply":"2024-05-02T04:53:58.769283Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"path_of_csv_file= '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:54:00.150968Z","iopub.execute_input":"2024-05-02T04:54:00.151334Z","iopub.status.idle":"2024-05-02T04:54:00.155768Z","shell.execute_reply.started":"2024-05-02T04:54:00.151298Z","shell.execute_reply":"2024-05-02T04:54:00.154746Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_data_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:54:01.421489Z","iopub.execute_input":"2024-05-02T04:54:01.422412Z","iopub.status.idle":"2024-05-02T04:54:01.426876Z","shell.execute_reply.started":"2024-05-02T04:54:01.422367Z","shell.execute_reply":"2024-05-02T04:54:01.425894Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ntrain_dataset = SummarizationDataset(tokenizer, path_of_csv_file,max_length =512, use_percentage = 10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:54:04.431606Z","iopub.execute_input":"2024-05-02T04:54:04.432481Z","iopub.status.idle":"2024-05-02T04:54:30.404417Z","shell.execute_reply.started":"2024-05-02T04:54:04.432439Z","shell.execute_reply":"2024-05-02T04:54:30.403657Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test_dataset = SummarizationDataset(tokenizer, test_data_path, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:54:30.595856Z","iopub.execute_input":"2024-05-02T04:54:30.596184Z","iopub.status.idle":"2024-05-02T04:54:31.648419Z","shell.execute_reply.started":"2024-05-02T04:54:30.596154Z","shell.execute_reply":"2024-05-02T04:54:31.647636Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          # Output directory\n    num_train_epochs=3,              # Number of training epochs\n    per_device_train_batch_size=4,   # Batch size for training\n    per_device_eval_batch_size=4,    # Batch size for evaluation\n    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # Strength of weight decay\n    logging_dir='./logs',            # Directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n    save_strategy=\"epoch\"            # Save the model at the end of each epoch\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:56:02.811585Z","iopub.execute_input":"2024-05-02T04:56:02.812438Z","iopub.status.idle":"2024-05-02T04:56:02.845424Z","shell.execute_reply.started":"2024-05-02T04:56:02.812407Z","shell.execute_reply":"2024-05-02T04:56:02.844742Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:56:03.348980Z","iopub.execute_input":"2024-05-02T04:56:03.349828Z","iopub.status.idle":"2024-05-02T04:56:03.365588Z","shell.execute_reply.started":"2024-05-02T04:56:03.349798Z","shell.execute_reply":"2024-05-02T04:56:03.364683Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.init(mode=\"disabled\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:56:05.140781Z","iopub.execute_input":"2024-05-02T04:56:05.141483Z","iopub.status.idle":"2024-05-02T04:56:05.165392Z","shell.execute_reply.started":"2024-05-02T04:56:05.141451Z","shell.execute_reply":"2024-05-02T04:56:05.164383Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"# train the model \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:56:05.871013Z","iopub.execute_input":"2024-05-02T04:56:05.871777Z","iopub.status.idle":"2024-05-02T06:33:56.589373Z","shell.execute_reply.started":"2024-05-02T04:56:05.871746Z","shell.execute_reply":"2024-05-02T06:33:56.588492Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10767' max='10767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10767/10767 1:37:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.305300</td>\n      <td>0.295893</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.270600</td>\n      <td>0.294397</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.284900</td>\n      <td>0.293791</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10767, training_loss=0.4340459548896697, metrics={'train_runtime': 5870.0362, 'train_samples_per_second': 14.673, 'train_steps_per_second': 1.834, 'total_flos': 1.1657395386187776e+16, 'train_loss': 0.4340459548896697, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_metric","metadata":{"execution":{"iopub.status.busy":"2024-05-02T06:35:55.396833Z","iopub.execute_input":"2024-05-02T06:35:55.397199Z","iopub.status.idle":"2024-05-02T06:35:55.401988Z","shell.execute_reply.started":"2024-05-02T06:35:55.397170Z","shell.execute_reply":"2024-05-02T06:35:55.401032Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T06:35:22.972926Z","iopub.execute_input":"2024-05-02T06:35:22.973711Z","iopub.status.idle":"2024-05-02T06:35:39.793929Z","shell.execute_reply.started":"2024-05-02T06:35:22.973681Z","shell.execute_reply":"2024-05-02T06:35:39.792687Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f3304724dcc39a4b720906b3944144a078408571ef751ce1e3d54846735812c6\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":" rouge = load_metric('rouge')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T06:35:59.293946Z","iopub.execute_input":"2024-05-02T06:35:59.294867Z","iopub.status.idle":"2024-05-02T06:35:59.517019Z","shell.execute_reply.started":"2024-05-02T06:35:59.294831Z","shell.execute_reply":"2024-05-02T06:35:59.515992Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_rouge_scores(model, tokenizer, dataset):\n    # Determine the device to use, prefer GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)  # Ensure model is on the right device\n    \n    model.eval()\n    predictions = []\n    references = []\n    \n    for item in dataset:\n        with torch.no_grad():\n            # Ensure input tensors are on the same device as the model\n            input_ids = item['input_ids'].unsqueeze(0).to(device)\n            attention_mask = item['attention_mask'].unsqueeze(0).to(device)\n            \n            output = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=150,\n                num_beams=4,\n                length_penalty=2.0,\n                early_stopping=True\n            )\n            \n            decoded_pred = tokenizer.decode(output[0], skip_special_tokens=True)\n            decoded_ref = tokenizer.decode(item['labels'], skip_special_tokens=True)\n            \n            predictions.append(decoded_pred)\n            references.append(decoded_ref)\n    \n    # Use the ROUGE metric to compute scores\n    rouge = load_metric('rouge', trust_remote_code=True)\n    result = rouge.compute(predictions=predictions, references=references)\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T06:39:00.333220Z","iopub.execute_input":"2024-05-02T06:39:00.333874Z","iopub.status.idle":"2024-05-02T06:39:00.343246Z","shell.execute_reply.started":"2024-05-02T06:39:00.333841Z","shell.execute_reply":"2024-05-02T06:39:00.342312Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"rouge_scores = compute_rouge_scores(model, tokenizer, test_dataset)\nprint(\"ROUGE Scores:\", rouge_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T06:39:01.502905Z","iopub.execute_input":"2024-05-02T06:39:01.503270Z","iopub.status.idle":"2024-05-02T07:02:22.041821Z","shell.execute_reply.started":"2024-05-02T06:39:01.503239Z","shell.execute_reply":"2024-05-02T07:02:22.040775Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.3833178797696526, recall=0.4128924727064259, fmeasure=0.3861500212442018), mid=Score(precision=0.3914560345232244, recall=0.42153068870032934, fmeasure=0.39341562083191584), high=Score(precision=0.4001189942708382, recall=0.43013691234513945, fmeasure=0.40094289357063784)), 'rouge2': AggregateScore(low=Score(precision=0.17398937305010279, recall=0.18689932114767902, fmeasure=0.17485570169413817), mid=Score(precision=0.18140134357642762, recall=0.19466531000103598, fmeasure=0.18171760210706495), high=Score(precision=0.18901059819320015, recall=0.20238142247338914, fmeasure=0.18883279345111356)), 'rougeL': AggregateScore(low=Score(precision=0.2693763043488119, recall=0.2922786870337757, fmeasure=0.27215409409571073), mid=Score(precision=0.27671012792470284, recall=0.3004676294856073, fmeasure=0.2792645447937093), high=Score(precision=0.2843663165748895, recall=0.3085978865124499, fmeasure=0.2866285472270553)), 'rougeLsum': AggregateScore(low=Score(precision=0.26969870821345826, recall=0.2921911361345714, fmeasure=0.27202526975897834), mid=Score(precision=0.2765671792947808, recall=0.30024241687283104, fmeasure=0.2790907332209245), high=Score(precision=0.2848056333754418, recall=0.3086284786070951, fmeasure=0.2864998820949457))}\n","output_type":"stream"}]},{"cell_type":"code","source":"def simplified_rouge_scores(rouge_results):\n    # Extract only mid F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L\n    simplified_scores = {\n        'rouge1_fmeasure': rouge_results['rouge1'].mid.fmeasure,\n        'rouge2_fmeasure': rouge_results['rouge2'].mid.fmeasure,\n        'rougeL_fmeasure': rouge_results['rougeL'].mid.fmeasure\n    }\n    return simplified_scores\n\n# Assuming rouge_scores is the output from your previous compute_rouge_scores function\nsimplified_scores = simplified_rouge_scores(rouge_scores)\nprint(\"Simplified ROUGE Scores:\", simplified_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:12:13.065069Z","iopub.execute_input":"2024-05-02T07:12:13.065802Z","iopub.status.idle":"2024-05-02T07:12:13.072247Z","shell.execute_reply.started":"2024-05-02T07:12:13.065771Z","shell.execute_reply":"2024-05-02T07:12:13.071086Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Simplified ROUGE Scores: {'rouge1_fmeasure': 0.39341562083191584, 'rouge2_fmeasure': 0.18171760210706495, 'rougeL_fmeasure': 0.2792645447937093}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}