{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T08:13:26.300450Z","iopub.execute_input":"2024-05-06T08:13:26.301039Z","iopub.status.idle":"2024-05-06T08:13:46.979996Z","shell.execute_reply.started":"2024-05-06T08:13:26.301004Z","shell.execute_reply":"2024-05-06T08:13:46.979165Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-06 08:13:37.464006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 08:13:37.464097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 08:13:37.640004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\n\nclass SummarizationDataset(Dataset):\n    def __init__(self, tokenizer, file_path, max_length=512, use_percentage=10):\n        self.dataframe = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Sample a percentage of the data if use_percentage is less than 100\n        if use_percentage < 100:\n            self.dataframe = self.dataframe.sample(frac=use_percentage / 100.0, random_state=42).reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        article_text = row['article']\n        highlights_text = row['highlights']\n        \n        input_text = f\"summarize: {article_text}\"\n        source_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        target_encoding = self.tokenizer(\n            highlights_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': source_encoding['input_ids'].squeeze(),\n            'attention_mask': source_encoding['attention_mask'].squeeze(),\n            'labels': target_encoding['input_ids'].squeeze()\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:46.981422Z","iopub.execute_input":"2024-05-06T08:13:46.981966Z","iopub.status.idle":"2024-05-06T08:13:46.992031Z","shell.execute_reply.started":"2024-05-06T08:13:46.981940Z","shell.execute_reply":"2024-05-06T08:13:46.991167Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path_of_csv_file= '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:46.995665Z","iopub.execute_input":"2024-05-06T08:13:46.996017Z","iopub.status.idle":"2024-05-06T08:13:47.022716Z","shell.execute_reply.started":"2024-05-06T08:13:46.995989Z","shell.execute_reply":"2024-05-06T08:13:47.021860Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test_data_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:47.024733Z","iopub.execute_input":"2024-05-06T08:13:47.025051Z","iopub.status.idle":"2024-05-06T08:13:47.031951Z","shell.execute_reply.started":"2024-05-06T08:13:47.025026Z","shell.execute_reply":"2024-05-06T08:13:47.031179Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:47.032888Z","iopub.execute_input":"2024-05-06T08:13:47.033176Z","iopub.status.idle":"2024-05-06T08:13:52.487745Z","shell.execute_reply.started":"2024-05-06T08:13:47.033138Z","shell.execute_reply":"2024-05-06T08:13:52.486849Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66cd6334db924fee97178b93b6b8e496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef01d46dfefd4a71b344c372fbfe8aca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46472086054b4f44b597bdafac6313b7"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:52.489391Z","iopub.execute_input":"2024-05-06T08:13:52.489797Z","iopub.status.idle":"2024-05-06T08:13:53.820082Z","shell.execute_reply.started":"2024-05-06T08:13:52.489763Z","shell.execute_reply":"2024-05-06T08:13:53.819224Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd84cbaca91c4958a167ae78238712fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26eeb49b0dfb444381bed827c1bc09ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ee3a8752994042bc4dbae93d86d3dc"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load dataset\ntrain_dataset = SummarizationDataset(tokenizer, path_of_csv_file,max_length =512, use_percentage = 10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:13:53.821138Z","iopub.execute_input":"2024-05-06T08:13:53.821436Z","iopub.status.idle":"2024-05-06T08:14:24.027240Z","shell.execute_reply.started":"2024-05-06T08:13:53.821412Z","shell.execute_reply":"2024-05-06T08:14:24.026448Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_dataset = SummarizationDataset(tokenizer, test_data_path, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:24.211404Z","iopub.execute_input":"2024-05-06T08:14:24.211779Z","iopub.status.idle":"2024-05-06T08:14:25.296016Z","shell.execute_reply.started":"2024-05-06T08:14:24.211745Z","shell.execute_reply":"2024-05-06T08:14:25.295262Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Config","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:25.301325Z","iopub.execute_input":"2024-05-06T08:14:25.301580Z","iopub.status.idle":"2024-05-06T08:14:25.306289Z","shell.execute_reply.started":"2024-05-06T08:14:25.301557Z","shell.execute_reply":"2024-05-06T08:14:25.305289Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\nfrom torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert\n\n# class QuantizedT5(T5ForConditionalGeneration):\n#     def __init__(self, config):\n#         super().__init__(config)\n#         self.quant = QuantStub()\n#         self.dequant = DeQuantStub()\n\n#     def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None):\n#         input_ids = self.quant(input_ids)\n#         outputs = super().forward(input_ids, attention_mask=attention_mask,\n#                                   decoder_input_ids=decoder_input_ids, labels=labels)\n#         output = self.dequant(outputs.logits)\n#         return output\n\n# # # Initialize the model\n# # model = QuantizedT5.from_pretrained('t5-small')\n\n# # # Set the model to training mode\n# # model.train()\n\n# # # Prepare the model for QAT\n# # model = prepare_qat(model, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:51.315501Z","iopub.execute_input":"2024-05-06T08:14:51.315873Z","iopub.status.idle":"2024-05-06T08:14:51.321321Z","shell.execute_reply.started":"2024-05-06T08:14:51.315842Z","shell.execute_reply":"2024-05-06T08:14:51.320319Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class QuantizedT5(T5ForConditionalGeneration):\n    def __init__(self, config):\n        super().__init__(config)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n\n    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None, **kwargs):\n        input_ids = self.quant(input_ids)\n        # Pass along the additional kwargs to the superclass forward method\n        outputs = super().forward(input_ids, attention_mask=attention_mask,\n                                  decoder_input_ids=decoder_input_ids, labels=labels, **kwargs)\n        logits = self.dequant(outputs.logits)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:52.257974Z","iopub.execute_input":"2024-05-06T08:14:52.258684Z","iopub.status.idle":"2024-05-06T08:14:52.265618Z","shell.execute_reply.started":"2024-05-06T08:14:52.258648Z","shell.execute_reply":"2024-05-06T08:14:52.264592Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = QuantizedT5(T5Config.from_pretrained('t5-small'))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:54.911114Z","iopub.execute_input":"2024-05-06T08:14:54.911912Z","iopub.status.idle":"2024-05-06T08:14:56.335812Z","shell.execute_reply.started":"2024-05-06T08:14:54.911881Z","shell.execute_reply":"2024-05-06T08:14:56.334946Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:58.457055Z","iopub.execute_input":"2024-05-06T08:14:58.457429Z","iopub.status.idle":"2024-05-06T08:14:58.463247Z","shell.execute_reply.started":"2024-05-06T08:14:58.457399Z","shell.execute_reply":"2024-05-06T08:14:58.462358Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n#     \n#     fp16 = True,  # enable mixed precision it help speed up training\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n#     evaluation_strategy=\"no\",\n#     evaluation_strategy=\"steps\",\n    save_strategy=\"epoch\",\n#     load_best_model_at_end=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:58.851923Z","iopub.execute_input":"2024-05-06T08:14:58.852845Z","iopub.status.idle":"2024-05-06T08:14:58.978951Z","shell.execute_reply.started":"2024-05-06T08:14:58.852803Z","shell.execute_reply":"2024-05-06T08:14:58.977950Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:59.389325Z","iopub.execute_input":"2024-05-06T08:14:59.389665Z","iopub.status.idle":"2024-05-06T08:14:59.394760Z","shell.execute_reply.started":"2024-05-06T08:14:59.389638Z","shell.execute_reply":"2024-05-06T08:14:59.393624Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Define a split ratio for training and validation\ntrain_size = int(0.9 * len(train_dataset))  # 90% for training\neval_size = len(train_dataset) - train_size  # 10% for evaluation","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:14:59.804335Z","iopub.execute_input":"2024-05-06T08:14:59.804772Z","iopub.status.idle":"2024-05-06T08:14:59.809467Z","shell.execute_reply.started":"2024-05-06T08:14:59.804741Z","shell.execute_reply":"2024-05-06T08:14:59.808480Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Split the dataset\ntrain_subset, eval_subset = random_split(train_dataset, [train_size, eval_size])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:15:00.441279Z","iopub.execute_input":"2024-05-06T08:15:00.441930Z","iopub.status.idle":"2024-05-06T08:15:00.453805Z","shell.execute_reply.started":"2024-05-06T08:15:00.441895Z","shell.execute_reply":"2024-05-06T08:15:00.453049Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Assume `training_args` as you defined previously\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_subset,\n    eval_dataset=eval_subset\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:15:01.473396Z","iopub.execute_input":"2024-05-06T08:15:01.473752Z","iopub.status.idle":"2024-05-06T08:15:02.462939Z","shell.execute_reply.started":"2024-05-06T08:15:01.473724Z","shell.execute_reply":"2024-05-06T08:15:02.462162Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.init(mode=\"disabled\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:15:02.464401Z","iopub.execute_input":"2024-05-06T08:15:02.464682Z","iopub.status.idle":"2024-05-06T08:15:03.254463Z","shell.execute_reply.started":"2024-05-06T08:15:02.464657Z","shell.execute_reply":"2024-05-06T08:15:03.253591Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":" # Train and fine-tune with QAT\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:15:06.728095Z","iopub.execute_input":"2024-05-06T08:15:06.728812Z","iopub.status.idle":"2024-05-06T08:42:22.715943Z","shell.execute_reply.started":"2024-05-06T08:15:06.728780Z","shell.execute_reply":"2024-05-06T08:42:22.715101Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3230' max='3230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3230/3230 27:12, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>-2.277900</td>\n      <td>-2.285426</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3230, training_loss=-1.4252931433796145, metrics={'train_runtime': 1635.6526, 'train_samples_per_second': 15.797, 'train_steps_per_second': 1.975, 'total_flos': 3497096808235008.0, 'train_loss': -1.4252931433796145, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:35.952305Z","iopub.execute_input":"2024-05-06T08:48:35.953025Z","iopub.status.idle":"2024-05-06T08:48:51.425373Z","shell.execute_reply.started":"2024-05-06T08:48:35.952991Z","shell.execute_reply":"2024-05-06T08:48:51.424018Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0a4d74f5860b4751beec61fa1a5b4173e1920649ad8cf3d361cd99f47a55e587\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers = 4)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:51.427525Z","iopub.execute_input":"2024-05-06T08:48:51.427838Z","iopub.status.idle":"2024-05-06T08:48:51.433254Z","shell.execute_reply.started":"2024-05-06T08:48:51.427808Z","shell.execute_reply":"2024-05-06T08:48:51.432407Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:51.434339Z","iopub.execute_input":"2024-05-06T08:48:51.434605Z","iopub.status.idle":"2024-05-06T08:48:51.443633Z","shell.execute_reply.started":"2024-05-06T08:48:51.434581Z","shell.execute_reply":"2024-05-06T08:48:51.442764Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def compute_rouge_scores(model, dataloader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    predictions = []\n    references = []\n    rouge = load_metric('rouge', trust_remote_code=True)\n\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=150,\n                num_beams=4,\n                length_penalty=2.0,\n                early_stopping=True\n            )\n\n        decoded_preds = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n        decoded_refs = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n\n        predictions.extend(decoded_preds)\n        references.extend(decoded_refs)\n\n    result = rouge.compute(predictions=predictions, references=references)\n    return result\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:51.446653Z","iopub.execute_input":"2024-05-06T08:48:51.447460Z","iopub.status.idle":"2024-05-06T08:48:51.456674Z","shell.execute_reply.started":"2024-05-06T08:48:51.447432Z","shell.execute_reply":"2024-05-06T08:48:51.455793Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now call the function\nrouge_scores = compute_rouge_scores(model, test_dataloader)\nprint(\"ROUGE Scores:\", rouge_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:51.457659Z","iopub.execute_input":"2024-05-06T08:48:51.457909Z","iopub.status.idle":"2024-05-06T08:48:55.184738Z","shell.execute_reply.started":"2024-05-06T08:48:51.457887Z","shell.execute_reply":"2024-05-06T08:48:55.183202Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4134571692.py:8: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  rouge = load_metric('rouge', trust_remote_code=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e0cac9041645e8a66a79d5597903b6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now call the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rouge_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_rouge_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rouge_scores)\n","Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mcompute_rouge_scores\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m     26\u001b[0m decoded_refs \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(label, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1609\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1603\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1604\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1605\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1609\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3064\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3061\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3072\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: QuantizedT5.forward() missing 1 required positional argument: 'input_ids'"],"ename":"TypeError","evalue":"QuantizedT5.forward() missing 1 required positional argument: 'input_ids'","output_type":"error"}]},{"cell_type":"code","source":"def simplified_rouge_scores(rouge_results):\n    # Extract only mid F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L\n    simplified_scores = {\n        'rouge1_fmeasure': rouge_results['rouge1'].mid.fmeasure,\n        'rouge2_fmeasure': rouge_results['rouge2'].mid.fmeasure,\n        'rougeL_fmeasure': rouge_results['rougeL'].mid.fmeasure\n    }\n    return simplified_scores\n\n# Assuming rouge_scores is the output from your previous compute_rouge_scores function\nsimplified_scores = simplified_rouge_scores(rouge_scores)\nprint(\"Simplified ROUGE Scores:\", simplified_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T06:56:48.948985Z","iopub.status.idle":"2024-05-06T06:56:48.949353Z","shell.execute_reply.started":"2024-05-06T06:56:48.949172Z","shell.execute_reply":"2024-05-06T06:56:48.949186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}